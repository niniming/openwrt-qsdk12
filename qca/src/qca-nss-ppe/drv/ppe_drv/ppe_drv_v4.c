/*
 * Copyright (c) 2022 Qualcomm Innovation Center, Inc. All rights reserved.
 *
 * Permission to use, copy, modify, and/or distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 */

#include "ppe_drv.h"
#include "tun/ppe_drv_tun.h"
#include "tun/ppe_drv_tun_v4.h"
#include <net/vxlan.h>

/*
 * ppe_drv_fill_fse_v4_tuple_info()
 *	Fill FSE v4 tuple information
 */
static void ppe_drv_fill_fse_v4_tuple_info(struct ppe_drv_v4_conn_flow *conn, struct ppe_drv_fse_rule_info *fse_info, bool is_ds)
{
	struct ppe_drv_port *pp;

	fse_info->tuple.src_ip[0] = ppe_drv_v4_conn_flow_match_src_ip_get(conn);
	fse_info->tuple.src_port = ppe_drv_v4_conn_flow_match_src_ident_get(conn);
	fse_info->tuple.dest_ip[0] = ppe_drv_v4_conn_flow_match_dest_ip_get(conn);
	fse_info->tuple.dest_port = ppe_drv_v4_conn_flow_match_dest_ident_get(conn);
	fse_info->tuple.protocol = ppe_drv_v4_conn_flow_match_protocol_get(conn);

	pp = ppe_drv_v4_conn_flow_rx_port_get(conn);

	fse_info->dev = ppe_drv_port_to_dev(pp);
	fse_info->flags |= PPE_DRV_FSE_IPV4;
	if (is_ds) {
		fse_info->flags |= PPE_DRV_FSE_DS;
	}

	fse_info->vp_num = pp->port;

	ppe_drv_trace("src_ip: %x\n", fse_info->tuple.src_ip[0]);
	ppe_drv_trace("src_port: %x\n", fse_info->tuple.src_port);
	ppe_drv_trace("dest_ip: %x\n", fse_info->tuple.dest_ip[0]);
	ppe_drv_trace("dest_port: %x\n", fse_info->tuple.dest_port);
	ppe_drv_trace("protocol: %x\n", fse_info->tuple.protocol);
	ppe_drv_trace("dev: %s\n", fse_info->dev->name);
	ppe_drv_trace("vp_num: %d\n", fse_info->vp_num);
}

/*
 * ppe_drv_fse_interface_check()
 *	check if interface is FSE capable
 */
static bool ppe_drv_fse_interface_check(struct ppe_drv_v4_conn_flow *pcf)
{
	struct ppe_drv *p = &ppe_drv_gbl;
	struct ppe_drv_port *rx_port = ppe_drv_v4_conn_flow_rx_port_get(pcf);
	struct ppe_drv_port *tx_port = ppe_drv_v4_conn_flow_tx_port_get(pcf);
	bool is_tx_ds = (tx_port->user_type == PPE_DRV_PORT_USER_TYPE_DS);
	bool is_rx_ds = (rx_port->user_type == PPE_DRV_PORT_USER_TYPE_DS);
	bool is_tx_active_vp = (tx_port->user_type == PPE_DRV_PORT_USER_TYPE_ACTIVE_VP);
	bool is_rx_active_vp = (rx_port->user_type == PPE_DRV_PORT_USER_TYPE_ACTIVE_VP);

	/*
	 * If FSE operation is not enabled; return true and continue with a successfull
	 * PPE rule push
	 */
	if (!p->is_wifi_fse_up || !p->fse_enable || !p->fse_ops) {
		ppe_drv_trace("FSE operation not enabled: enable: %d ops: %p\n", p->fse_enable, p->fse_ops);
		return false;
	}

	/*
	 * If interfaces are not Wi-FI VPs then return true and continue with a successfull
	 * PPE rule push
	 */
	if (!is_tx_ds && !is_rx_ds && !is_tx_active_vp && !is_rx_active_vp) {
		ppe_drv_trace("no active or ds vp\n");
		return false;
	}

	/*
	 * TODO: Handle Inter-VAP FSE rule push as a seperate patch
	 */
	if (is_tx_ds && is_rx_ds) {
		ppe_drv_trace("Inter VAP FSE rule push not enabled for DS VP\n");
		return false;
	}

	/*
	 * TODO: Handle Inter-VAP FSE rule push as a seperate patch
	 */
	if (is_tx_active_vp && is_rx_active_vp) {
		ppe_drv_trace("Inter VAP FSE rule push not enabled for active VP\n");
		return false;
	}

	return true;
}

void ppe_drv_v4_flow_vlan_set(struct ppe_drv_v4_conn_flow *pcf,
			      uint32_t primary_ingress_vlan_tag, uint32_t primary_egress_vlan_tag,
			      uint32_t secondary_ingress_vlan_tag, uint32_t secondary_egress_vlan_tag)
{
	if ((primary_ingress_vlan_tag & PPE_DRV_VLAN_ID_MASK) != PPE_DRV_VLAN_NOT_CONFIGURED) {
		pcf->ingress_vlan[0].tpid = primary_ingress_vlan_tag >> 16;
		pcf->ingress_vlan[0].tci = (uint16_t) primary_ingress_vlan_tag;
		pcf->ingress_vlan_cnt++;

		/*
		 * Check if flow needs 802.1p marking.
		 */
		if (pcf->ingress_vlan[0].tci & PPE_DRV_VLAN_PRIORITY_MASK) {
			ppe_drv_v4_conn_flow_flags_set(pcf, PPE_DRV_V4_CONN_FLOW_FLAG_VLAN_PRI_MARKING);
		}
	}

	if ((secondary_ingress_vlan_tag & PPE_DRV_VLAN_ID_MASK) != PPE_DRV_VLAN_NOT_CONFIGURED) {
		pcf->ingress_vlan[1].tpid = secondary_ingress_vlan_tag >> 16;
		pcf->ingress_vlan[1].tci = (uint16_t) secondary_ingress_vlan_tag;
		pcf->ingress_vlan_cnt++;

		/*
		 * Check if flow needs 802.1p marking.
		 */
		if (pcf->ingress_vlan[1].tci & PPE_DRV_VLAN_PRIORITY_MASK) {
			ppe_drv_v4_conn_flow_flags_set(pcf, PPE_DRV_V4_CONN_FLOW_FLAG_VLAN_PRI_MARKING);
		}
	}

	if ((primary_egress_vlan_tag & PPE_DRV_VLAN_ID_MASK) != PPE_DRV_VLAN_NOT_CONFIGURED) {
		pcf->egress_vlan[0].tpid = primary_egress_vlan_tag >> 16;
		pcf->egress_vlan[0].tci = (uint16_t) primary_egress_vlan_tag;
		pcf->egress_vlan_cnt++;

		/*
		 * Check if flow needs 802.1p marking.
		 */
		if (pcf->egress_vlan[0].tci & PPE_DRV_VLAN_PRIORITY_MASK) {
			ppe_drv_v4_conn_flow_flags_set(pcf, PPE_DRV_V4_CONN_FLOW_FLAG_VLAN_PRI_MARKING);
		}
	}

	if ((secondary_egress_vlan_tag & PPE_DRV_VLAN_ID_MASK) != PPE_DRV_VLAN_NOT_CONFIGURED) {
		pcf->egress_vlan[1].tpid = ((secondary_egress_vlan_tag & PPE_DRV_VLAN_TPID_MASK) >> 16);
		pcf->egress_vlan[1].tci = (secondary_egress_vlan_tag & PPE_DRV_VLAN_TCI_MASK);
		pcf->egress_vlan_cnt++;

		/*
		 * Check if flow needs 802.1p marking.
		 */
		if (pcf->egress_vlan[1].tci & PPE_DRV_VLAN_PRIORITY_MASK) {
			ppe_drv_v4_conn_flow_flags_set(pcf, PPE_DRV_V4_CONN_FLOW_FLAG_VLAN_PRI_MARKING);
		}
	}
}

/*
 * ppe_drv_v4_rfs_conn_fill()
 *	Populate single direction flow object rule.
 */
ppe_drv_ret_t ppe_drv_v4_rfs_conn_fill(struct ppe_drv_v4_rule_create *create, struct ppe_drv_v4_conn *cn,
				   enum ppe_drv_conn_type flow_type)
{
	struct ppe_drv_v4_connection_rule *conn = &create->conn_rule;
	struct ppe_drv_v4_5tuple *tuple = &create->tuple;
	struct ppe_drv_iface *if_rx, *if_tx;
	struct ppe_drv_v4_conn_flow *pcf = &cn->pcf;
	uint16_t rule_flags = create->rule_flags;
	struct ppe_drv_comm_stats *comm_stats;
	struct ppe_drv_port *pp_rx, *pp_tx;
	struct ppe_drv *p = &ppe_drv_gbl;

	comm_stats = &p->stats.comm_stats[flow_type];

	/*
	 * Make sure both Rx and Tx inteface are mapped to PPE ports properly.
	 */
	if_rx = ppe_drv_iface_get_by_idx(conn->rx_if);
	if (!if_rx) {
		ppe_drv_stats_inc(&comm_stats->v4_create_rfs_fail_invalid_rx_if);
		ppe_drv_warn("%p: No PPE interface corresponding to rx_if: %d", create, conn->rx_if);
		return PPE_DRV_RET_FAILURE_INVALID_PARAM;
	}

	pp_rx = ppe_drv_iface_port_get(if_rx);
	if (!pp_rx) {
		ppe_drv_stats_inc(&comm_stats->v4_create_rfs_fail_invalid_rx_port);
		ppe_drv_warn("%p: Invalid Rx IF: %d", create, conn->rx_if);
		return PPE_DRV_RET_FAILURE_IFACE_PORT_MAP;
	}

	if_tx = ppe_drv_iface_get_by_idx(conn->tx_if);
	if (!if_tx) {
		ppe_drv_stats_inc(&comm_stats->v4_create_rfs_fail_invalid_tx_if);
		ppe_drv_warn("%p: No PPE interface corresponding to tx_if: %d", create, conn->tx_if);
		return PPE_DRV_RET_FAILURE_INVALID_PARAM;
	}

	pp_tx = ppe_drv_iface_port_get(if_tx);
	if (!pp_tx) {
		ppe_drv_stats_inc(&comm_stats->v4_create_rfs_fail_invalid_tx_port);
		ppe_drv_warn("%p: Invalid Tx IF: %d", create, conn->tx_if);
		return PPE_DRV_RET_FAILURE_IFACE_PORT_MAP;
	}

	/*
	 * Set the egress point based on direction of the flow
	 * TODO: Handle the else case and add error counter for it
	 */
	if ((pp_tx->flags & PPE_DRV_PORT_RFS_ENABLED) && (pp_tx->user_type == PPE_DRV_PORT_USER_TYPE_PASSIVE_VP)) {
		pcf->eg_port_if = ppe_drv_iface_ref(if_tx);
	} else if ((pp_rx->flags & PPE_DRV_PORT_RFS_ENABLED) && (pp_rx->user_type == PPE_DRV_PORT_USER_TYPE_PASSIVE_VP)) {
		pcf->eg_port_if = ppe_drv_iface_ref(if_rx);
	}

	/*
	 * Bridge flow
	 */
	if (rule_flags & PPE_DRV_V4_RULE_FLAG_BRIDGE_FLOW) {
		ppe_drv_v4_conn_flow_flags_set(pcf, PPE_DRV_V4_CONN_FLOW_FLAG_BRIDGE_FLOW);
	}

	ppe_drv_v4_conn_flow_conn_set(pcf, cn);

	/*
	 * Set Rx and Tx port.
	 */
	ppe_drv_v4_conn_flow_rx_port_set(pcf, pp_rx);
	ppe_drv_v4_conn_flow_tx_port_set(pcf, pp_tx);

	/*
	 * Set 5-tuple along with SNAT/DNAT requirement.
	 */
	ppe_drv_v4_conn_flow_match_protocol_set(pcf, tuple->protocol);
	ppe_drv_v4_conn_flow_match_src_ip_set(pcf, tuple->flow_ip);
	ppe_drv_v4_conn_flow_match_src_ident_set(pcf, tuple->flow_ident);
	ppe_drv_v4_conn_flow_match_dest_ip_set(pcf, tuple->return_ip);
	ppe_drv_v4_conn_flow_match_dest_ident_set(pcf, tuple->return_ident);
	ppe_drv_v4_conn_flow_xlate_src_ip_set(pcf, conn->flow_ip_xlate);
	ppe_drv_v4_conn_flow_xlate_src_ident_set(pcf, conn->flow_ident_xlate);
	ppe_drv_v4_conn_flow_xlate_dest_ip_set(pcf, conn->return_ip_xlate);
	ppe_drv_v4_conn_flow_xlate_dest_ident_set(pcf, conn->return_ident_xlate);

	/*
	 * Flow MTU and transmit MAC address.
	 */
	ppe_drv_v4_conn_flow_xmit_interface_mtu_set(pcf, conn->flow_mtu);

	return PPE_DRV_RET_SUCCESS;
}

/*
 * ppe_drv_v4_conn_fill()
 *	Populate each direction flow object.
 */
ppe_drv_ret_t ppe_drv_v4_conn_fill(struct ppe_drv_v4_rule_create *create, struct ppe_drv_v4_conn *cn,
				   enum ppe_drv_conn_type flow_type)
{
	struct ppe_drv_v4_connection_rule *conn = &create->conn_rule;
	struct ppe_drv_v4_5tuple *tuple = &create->tuple;
	struct ppe_drv_pppoe_session *flow_pppoe_rule = &create->pppoe_rule.flow_session;
	struct ppe_drv_pppoe_session *return_pppoe_rule = &create->pppoe_rule.return_session;
	struct ppe_drv_dscp_rule *dscp_rule = &create->dscp_rule;
	struct ppe_drv_vlan_info *vlan_primary_rule = &create->vlan_rule.primary_vlan;
	struct ppe_drv_vlan_info *vlan_secondary_rule = &create->vlan_rule.secondary_vlan;
	struct ppe_drv_iface *if_rx, *if_tx, *top_if_rx, *top_if_tx;
	struct ppe_drv_top_if_rule *top_rule = &create->top_rule;
	struct ppe_drv_service_class_rule *sawf_rule = &create->sawf_rule;
	struct ppe_drv_qos_rule *qos_rule = &create->qos_rule;
	struct ppe_drv_v4_conn_flow *pcf = &cn->pcf;
	struct ppe_drv_v4_conn_flow *pcr = &cn->pcr;
	uint16_t valid_flags = create->valid_flags;
	uint16_t rule_flags = create->rule_flags;
	uint32_t sawf_tag;
	struct ppe_drv_comm_stats *comm_stats;
	struct ppe_drv_port *pp_rx, *pp_tx;
	struct ppe_drv *p = &ppe_drv_gbl;

	comm_stats = &p->stats.comm_stats[flow_type];
	/*
	 * Make sure both Rx and Tx inteface are mapped to PPE ports properly.
	 */
	if_rx = ppe_drv_iface_get_by_idx(conn->rx_if);
	if (!if_rx) {
		ppe_drv_stats_inc(&comm_stats->v4_create_fail_invalid_rx_if);
		ppe_drv_warn("%p: No PPE interface corresponding to rx_if: %d", create, conn->rx_if);
		return PPE_DRV_RET_FAILURE_INVALID_PARAM;
	}

	pp_rx = ppe_drv_iface_port_get(if_rx);
	if (!pp_rx) {
		ppe_drv_stats_inc(&comm_stats->v4_create_fail_invalid_rx_port);
		ppe_drv_warn("%p: Invalid Rx IF: %d", create, conn->rx_if);
		return PPE_DRV_RET_FAILURE_IFACE_PORT_MAP;
	}

	if_tx = ppe_drv_iface_get_by_idx(conn->tx_if);
	if (!if_tx) {
		ppe_drv_stats_inc(&comm_stats->v4_create_fail_invalid_tx_if);
		ppe_drv_warn("%p: No PPE interface corresponding to tx_if: %d", create, conn->tx_if);
		return PPE_DRV_RET_FAILURE_INVALID_PARAM;
	}

	pp_tx = ppe_drv_iface_port_get(if_tx);
	if (!pp_tx) {
		ppe_drv_stats_inc(&comm_stats->v4_create_fail_invalid_tx_port);
		ppe_drv_warn("%p: Invalid Tx IF: %d", create, conn->tx_if);
		return PPE_DRV_RET_FAILURE_IFACE_PORT_MAP;
	}

	top_if_rx = ppe_drv_iface_get_by_idx(top_rule->rx_if);
	if (!top_if_rx) {
		ppe_drv_stats_inc(&comm_stats->v4_create_fail_invalid_rx_if);
		ppe_drv_warn("%p: No PPE interface corresponding to rx_if: %d", create, top_rule->rx_if);
		return PPE_DRV_RET_FAILURE_INVALID_PARAM;
	}

	top_if_tx = ppe_drv_iface_get_by_idx(top_rule->tx_if);
	if (!top_if_tx) {
		ppe_drv_stats_inc(&comm_stats->v4_create_fail_invalid_tx_if);
		ppe_drv_warn("%p: No PPE interface corresponding to tx_if: %d", create, top_rule->tx_if);
		return PPE_DRV_RET_FAILURE_INVALID_PARAM;
	}

	/*
	 * For DS flow if user type is not DS return invalid user type.
	 */
	if (rule_flags & PPE_DRV_V4_RULE_FLAG_DS_FLOW) {
		if ((pp_tx->user_type != PPE_DRV_PORT_USER_TYPE_DS) && (pp_rx->user_type != PPE_DRV_PORT_USER_TYPE_DS)) {
			ppe_drv_warn("%p: Invalid user type: %d", create, pp_tx->user_type);
			return PPE_DRV_RET_INVALID_USER_TYPE;
		}
	}

	/*
	 * Bridge flow
	 */
	if (rule_flags & PPE_DRV_V4_RULE_FLAG_BRIDGE_FLOW) {
		/*
		 * Bridge flow with NAT?
		 */
		if ((tuple->flow_ip != conn->flow_ip_xlate) || (tuple->return_ip != conn->return_ip_xlate)) {
			ppe_drv_stats_inc(&comm_stats->v4_create_fail_bridge_nat);
			ppe_drv_warn("%p: NAT not support with bridge flows! rule_flags: 0x%x "
					"flow_ip: %pI4 flow_ip_xlate: %pI4 return_ip: %pI4 return_ip_xlate: %pI4",
					create, rule_flags, &tuple->flow_ip, &conn->flow_ip_xlate,
					&tuple->return_ip, &conn->return_ip_xlate);
			return PPE_DRV_RET_FAILURE_BRIDGE_NAT;
		}

		ppe_drv_v4_conn_flow_flags_set(pcf, PPE_DRV_V4_CONN_FLOW_FLAG_BRIDGE_FLOW);
		ppe_drv_v4_conn_flow_flags_set(pcr, PPE_DRV_V4_CONN_FLOW_FLAG_BRIDGE_FLOW);
	}

	/*
	 * Note: PPE can't support both SNAT and DNAT simultaneously.
	 */
	if ((tuple->flow_ip != conn->flow_ip_xlate) && (tuple->return_ip != conn->return_ip_xlate)) {
		ppe_drv_stats_inc(&comm_stats->v4_create_fail_snat_dnat);
		ppe_drv_warn("%p: Invalid Tx IF: %d", create, conn->tx_if);
		return PPE_DRV_RET_FAILURE_SNAT_DNAT_SIMUL;
	}

	/*
	 * Prepare flow direction rule
	 */
	if (rule_flags & PPE_DRV_V4_RULE_FLAG_FLOW_VALID) {
		ppe_drv_v4_conn_flow_conn_set(pcf, cn);

		/*
		 * Set Rx and Tx port.
		 */
		ppe_drv_v4_conn_flow_rx_port_set(pcf, pp_rx);
		ppe_drv_v4_conn_flow_tx_port_set(pcf, pp_tx);

		/*
		 * Set 5-tuple along with SNAT/DNAT requirement.
		 */
		ppe_drv_v4_conn_flow_match_protocol_set(pcf, tuple->protocol);
		ppe_drv_v4_conn_flow_match_src_ip_set(pcf, tuple->flow_ip);
		ppe_drv_v4_conn_flow_match_src_ident_set(pcf, tuple->flow_ident);
		ppe_drv_v4_conn_flow_match_dest_ip_set(pcf, tuple->return_ip);
		ppe_drv_v4_conn_flow_match_dest_ident_set(pcf, tuple->return_ident);
		ppe_drv_v4_conn_flow_xlate_src_ip_set(pcf, conn->flow_ip_xlate);
		ppe_drv_v4_conn_flow_xlate_src_ident_set(pcf, conn->flow_ident_xlate);
		ppe_drv_v4_conn_flow_xlate_dest_ip_set(pcf, conn->return_ip_xlate);
		ppe_drv_v4_conn_flow_xlate_dest_ident_set(pcf, conn->return_ident_xlate);

		if ((ppe_drv_v4_conn_flow_match_src_ip_get(pcf) != ppe_drv_v4_conn_flow_xlate_src_ip_get(pcf))
			|| (ppe_drv_v4_conn_flow_match_src_ident_get(pcf) != ppe_drv_v4_conn_flow_xlate_src_ident_get(pcf))) {
			ppe_drv_v4_conn_flow_flags_set(pcf, PPE_DRV_V4_CONN_FLOW_FLAG_XLATE_SRC);
		}

		if ((ppe_drv_v4_conn_flow_match_dest_ip_get(pcf) != ppe_drv_v4_conn_flow_xlate_dest_ip_get(pcf))
			|| (ppe_drv_v4_conn_flow_match_dest_ident_get(pcf) != ppe_drv_v4_conn_flow_xlate_dest_ident_get(pcf))) {
			ppe_drv_v4_conn_flow_flags_set(pcf, PPE_DRV_V4_CONN_FLOW_FLAG_XLATE_DEST);
		}

		/*
		 * Flow MTU and transmit MAC address.
		 */
		ppe_drv_v4_conn_flow_xmit_interface_mtu_set(pcf, conn->return_mtu);
		ppe_drv_v4_conn_flow_xmit_dest_mac_addr_set(pcf, conn->return_mac);

		if (valid_flags & PPE_DRV_V4_VALID_FLAG_DSCP_MARKING) {
			ppe_drv_v4_conn_flow_egress_dscp_set(pcf, dscp_rule->flow_dscp);
			ppe_drv_v4_conn_flow_flags_set(pcf, PPE_DRV_V4_CONN_FLOW_FLAG_DSCP_MARKING);
		}

		/*
		 * For VP flow if user type is DS, set conn rule VP valid.
		 */
		if ((rule_flags & PPE_DRV_V4_RULE_FLAG_VP_FLOW) && (pp_tx->user_type == PPE_DRV_PORT_USER_TYPE_DS)) {
			ppe_drv_v4_conn_flow_flags_set(pcf, PPE_DRV_V4_CONN_FLAG_FLOW_VP_VALID);
		}

		/*
		 * Check if SAWF info is valid in this direction and if the
		 * interface is a wifi VP.
		 */
		if ((valid_flags & PPE_DRV_V4_VALID_FLAG_SAWF) &&
					(ppe_drv_port_flags_check(pp_tx, PPE_DRV_PORT_FLAG_WIFI_DEV))) {
			sawf_tag = PPE_DRV_SAWF_TAG_GET(sawf_rule->flow_mark);
			if (sawf_tag == PPE_DRV_SAWF_VALID_TAG) {
				ppe_drv_v4_conn_flow_sawf_set(pcf, sawf_rule->flow_mark);
				ppe_drv_v4_conn_flow_flags_set(pcf, PPE_DRV_V4_CONN_FLOW_FLAG_SAWF_MARKING);

				if (valid_flags & PPE_DRV_V4_VALID_FLAG_QOS) {
					ppe_drv_v4_conn_flow_int_pri_set(pcf, qos_rule->flow_qos_tag);
					ppe_drv_v4_conn_flow_flags_set(pcf, PPE_DRV_V4_CONN_FLOW_FLAG_QOS_VALID);
				}
			}
		}

		if (valid_flags & PPE_DRV_V4_VALID_FLAG_VLAN) {
			pcf->ingress_vlan[0].tci = PPE_DRV_VLAN_NOT_CONFIGURED;
			pcf->ingress_vlan[1].tci = PPE_DRV_VLAN_NOT_CONFIGURED;
			pcf->egress_vlan[0].tci = PPE_DRV_VLAN_NOT_CONFIGURED;
			pcf->egress_vlan[1].tci = PPE_DRV_VLAN_NOT_CONFIGURED;
			ppe_drv_v4_flow_vlan_set(pcf, vlan_primary_rule->ingress_vlan_tag,
					vlan_primary_rule->egress_vlan_tag,
					vlan_secondary_rule->ingress_vlan_tag,
					vlan_secondary_rule->egress_vlan_tag);
		}

		if (valid_flags & PPE_DRV_V4_VALID_FLAG_RETURN_PPPOE) {
			ppe_drv_v4_conn_flow_pppoe_session_id_set(pcf, return_pppoe_rule->session_id);
			ppe_drv_v4_conn_flow_pppoe_server_mac_set(pcf, return_pppoe_rule->server_mac);
			ppe_drv_v4_conn_flow_flags_set(pcf, PPE_DRV_V4_CONN_FLOW_FLAG_PPPOE_FLOW);
		}

		/*
		 * Bridge + VLAN? Make sure both top interfaces are attached to same parent.
		 */
		if ((rule_flags & PPE_DRV_V4_RULE_FLAG_BRIDGE_FLOW)
		       && (ppe_drv_v4_conn_flow_ingress_vlan_cnt_get(pcf)
			|| ppe_drv_v4_conn_flow_egress_vlan_cnt_get(pcf))) {

			if (ppe_drv_iface_parent_get(top_if_rx) != ppe_drv_iface_parent_get(top_if_tx)) {
				ppe_drv_stats_inc(&comm_stats->v4_create_fail_vlan_filter);
				ppe_drv_warn("%p: IF not part of same bridge rx_if: %d tx_if: %d",
						create, top_rule->rx_if, top_rule->tx_if);
				return PPE_DRV_RET_FAILURE_NOT_BRIDGE_SLAVES;
			}
		}

		/*
		 * Check if destination vp is inline EIP virtual port.
		 */
		if (ppe_drv_port_flags_check(ppe_drv_v4_conn_flow_tx_port_get(pcf), PPE_DRV_PORT_FLAG_IIPSEC)) {
			ppe_drv_v4_conn_flow_flags_set(pcf, PPE_DRV_V4_CONN_FLOW_FLAG_INLINE_IPSEC);
		}
	}

	/*
	 * Prepare return direction rule
	 */
	if (rule_flags & PPE_DRV_V4_RULE_FLAG_RETURN_VALID) {
		ppe_drv_v4_conn_flow_conn_set(pcr, cn);

		/*
		 * Set Rx and Tx port.
		 */
		ppe_drv_v4_conn_flow_rx_port_set(pcr, pp_tx);
		ppe_drv_v4_conn_flow_tx_port_set(pcr, pp_rx);

		/*
		 * Set 5-tuple along with SNAT/DNAT requirement.
		 */
		ppe_drv_v4_conn_flow_match_protocol_set(pcr, tuple->protocol);
		ppe_drv_v4_conn_flow_match_src_ip_set(pcr, conn->return_ip_xlate);
		ppe_drv_v4_conn_flow_match_src_ident_set(pcr, conn->return_ident_xlate);
		ppe_drv_v4_conn_flow_match_dest_ip_set(pcr, conn->flow_ip_xlate);
		ppe_drv_v4_conn_flow_match_dest_ident_set(pcr, conn->flow_ident_xlate);
		ppe_drv_v4_conn_flow_xlate_src_ip_set(pcr, tuple->return_ip);
		ppe_drv_v4_conn_flow_xlate_src_ident_set(pcr, tuple->return_ident);
		ppe_drv_v4_conn_flow_xlate_dest_ip_set(pcr, tuple->flow_ip);
		ppe_drv_v4_conn_flow_xlate_dest_ident_set(pcr, tuple->flow_ident);

		if ((ppe_drv_v4_conn_flow_match_src_ip_get(pcr) != ppe_drv_v4_conn_flow_xlate_src_ip_get(pcr))
			|| (ppe_drv_v4_conn_flow_match_src_ident_get(pcr) != ppe_drv_v4_conn_flow_xlate_src_ident_get(pcr))) {
			ppe_drv_v4_conn_flow_flags_set(pcr, PPE_DRV_V4_CONN_FLOW_FLAG_XLATE_SRC);
		}

		if ((ppe_drv_v4_conn_flow_match_dest_ip_get(pcr) != ppe_drv_v4_conn_flow_xlate_dest_ip_get(pcr))
			|| (ppe_drv_v4_conn_flow_match_dest_ident_get(pcr) != ppe_drv_v4_conn_flow_xlate_dest_ident_get(pcr))) {
			ppe_drv_v4_conn_flow_flags_set(pcr, PPE_DRV_V4_CONN_FLOW_FLAG_XLATE_DEST);
		}

		/*
		 * Flow MTU and transmit MAC address.
		 */
		ppe_drv_v4_conn_flow_xmit_interface_mtu_set(pcr, conn->flow_mtu);
		ppe_drv_v4_conn_flow_xmit_dest_mac_addr_set(pcr, conn->flow_mac);

		if (valid_flags & PPE_DRV_V4_VALID_FLAG_DSCP_MARKING) {
			ppe_drv_v4_conn_flow_egress_dscp_set(pcr, dscp_rule->return_dscp);
			ppe_drv_v4_conn_flow_flags_set(pcr, PPE_DRV_V4_CONN_FLOW_FLAG_DSCP_MARKING);
		}

		/*
		 * For VP flow if user type is DS, set conn rule VP valid.
		 */
		if ((rule_flags & PPE_DRV_V4_RULE_FLAG_VP_FLOW) && (pp_rx->user_type == PPE_DRV_PORT_USER_TYPE_DS)) {
			ppe_drv_v4_conn_flow_flags_set(pcr, PPE_DRV_V4_CONN_FLAG_FLOW_VP_VALID);
		}

		/*
		 * Check if SAWF info is valid in this direction and if the
		 * interface is a wifi VP.
		 */
		if ((valid_flags & PPE_DRV_V4_VALID_FLAG_SAWF) &&
					(ppe_drv_port_flags_check(pp_rx, PPE_DRV_PORT_FLAG_WIFI_DEV))) {
			sawf_tag = PPE_DRV_SAWF_TAG_GET(sawf_rule->return_mark);
			if (sawf_tag == PPE_DRV_SAWF_VALID_TAG) {
				ppe_drv_v4_conn_flow_sawf_set(pcr, sawf_rule->return_mark);
				ppe_drv_v4_conn_flow_flags_set(pcr, PPE_DRV_V4_CONN_FLOW_FLAG_SAWF_MARKING);

				if (valid_flags & PPE_DRV_V4_VALID_FLAG_QOS) {
					ppe_drv_v4_conn_flow_int_pri_set(pcr, qos_rule->return_qos_tag);
					ppe_drv_v4_conn_flow_flags_set(pcr, PPE_DRV_V4_CONN_FLOW_FLAG_QOS_VALID);
				}
			}
		}

		if (valid_flags & PPE_DRV_V4_VALID_FLAG_VLAN) {
			pcr->ingress_vlan[0].tci = PPE_DRV_VLAN_NOT_CONFIGURED;
			pcr->ingress_vlan[1].tci = PPE_DRV_VLAN_NOT_CONFIGURED;
			pcr->egress_vlan[0].tci = PPE_DRV_VLAN_NOT_CONFIGURED;
			pcr->egress_vlan[1].tci = PPE_DRV_VLAN_NOT_CONFIGURED;
			ppe_drv_v4_flow_vlan_set(pcr, vlan_primary_rule->egress_vlan_tag,
					vlan_primary_rule->ingress_vlan_tag,
					vlan_secondary_rule->egress_vlan_tag,
					vlan_secondary_rule->ingress_vlan_tag);
		}

		if (valid_flags & PPE_DRV_V4_VALID_FLAG_FLOW_PPPOE) {
			ppe_drv_v4_conn_flow_pppoe_session_id_set(pcr, flow_pppoe_rule->session_id);
			ppe_drv_v4_conn_flow_pppoe_server_mac_set(pcr, flow_pppoe_rule->server_mac);
			ppe_drv_v4_conn_flow_flags_set(pcr, PPE_DRV_V4_CONN_FLOW_FLAG_PPPOE_FLOW);
		}

		/*
		 * Bridge + VLAN? Make sure both top interfaces are attached to same parent.
		 */
		if ((rule_flags & PPE_DRV_V4_RULE_FLAG_BRIDGE_FLOW)
		       && (ppe_drv_v4_conn_flow_ingress_vlan_cnt_get(pcr)
			|| ppe_drv_v4_conn_flow_egress_vlan_cnt_get(pcr))) {

			if (ppe_drv_iface_parent_get(top_if_rx) != ppe_drv_iface_parent_get(top_if_tx)) {
				ppe_drv_stats_inc(&comm_stats->v4_create_fail_vlan_filter);
				ppe_drv_warn("%p: IF not part of same bridge rx_if: %d tx_if: %d",
						create, top_rule->rx_if, top_rule->tx_if);
				return PPE_DRV_RET_FAILURE_NOT_BRIDGE_SLAVES;
			}
		}

		/*
		 * Check if destination vp is inline EIP virtual port.
		 */
		if (ppe_drv_port_flags_check(ppe_drv_v4_conn_flow_tx_port_get(pcr), PPE_DRV_PORT_FLAG_IIPSEC)) {
			ppe_drv_v4_conn_flow_flags_set(pcr, PPE_DRV_V4_CONN_FLOW_FLAG_INLINE_IPSEC);
		}

		ppe_drv_v4_conn_flags_set(cn, PPE_DRV_V4_CONN_FLAG_RETURN_VALID);
	}

	return PPE_DRV_RET_SUCCESS;
}

/*
 * ppe_drv_v4_if_walk_release()
 *	Release references taken during interface hierarchy walk.
 */
void ppe_drv_v4_if_walk_release(struct ppe_drv_v4_conn_flow *pcf)
{
	if (pcf->eg_vsi_if) {
		ppe_drv_iface_deref_internal(pcf->eg_vsi_if);
		pcf->eg_vsi_if = NULL;
	}

	if (pcf->eg_l3_if) {
		ppe_drv_iface_deref_internal(pcf->eg_l3_if);
		pcf->eg_l3_if = NULL;
	}

	if (pcf->eg_port_if) {
		ppe_drv_iface_deref_internal(pcf->eg_port_if);
		pcf->eg_port_if = NULL;
	}
}

/*
 * ppe_drv_v4_if_walk()
 *	Walk iface heirarchy to obtain egress L3_IF and VSI
 */
bool ppe_drv_v4_if_walk(struct ppe_drv_v4_conn_flow *pcf, struct ppe_drv_top_if_rule *top_if, ppe_drv_iface_t tx_if)
{
	struct ppe_drv *p = &ppe_drv_gbl;
	struct ppe_drv_iface *eg_vsi_if = NULL;
	struct ppe_drv_iface *eg_l3_if = NULL;
	struct ppe_drv_iface *iface, *top_iface = NULL;
	struct ppe_drv_iface *tx_port_if = NULL;
	struct ppe_drv_vsi *vlan_vsi;
	struct ppe_drv_l3_if *pppoe_l3_if;
	uint32_t egress_vlan_inner = PPE_DRV_VLAN_NOT_CONFIGURED, egress_vlan_outer = PPE_DRV_VLAN_NOT_CONFIGURED;
	uint8_t vlan_cnt = ppe_drv_v4_conn_flow_egress_vlan_cnt_get(pcf);

	switch (vlan_cnt) {
	case 2:
		egress_vlan_inner = ppe_drv_v4_conn_flow_egress_vlan_get(pcf, 1)->tci & PPE_DRV_VLAN_ID_MASK;
		egress_vlan_outer = ppe_drv_v4_conn_flow_egress_vlan_get(pcf, 0)->tci & PPE_DRV_VLAN_ID_MASK;
		break;
	case 1:
		egress_vlan_inner = ppe_drv_v4_conn_flow_egress_vlan_get(pcf, 0)->tci & PPE_DRV_VLAN_ID_MASK;
		break;
	case 0:
		break;
	default:
		return false;
	}

	/*
	 * Should have a valid top interface.
	 */
	tx_port_if = ppe_drv_iface_get_by_idx(tx_if);
	if (!tx_port_if) {
		ppe_drv_warn("%p: No PPE interface corresponding to tx_if: %d", p, tx_if);
		return false;
	}

	/*
	 * if it's a bridge flow, hierarchy walk not needed.
	 */
	if (ppe_drv_v4_conn_flow_flags_check(pcf, PPE_DRV_V4_CONN_FLOW_FLAG_BRIDGE_FLOW)) {
		ppe_drv_v4_conn_flow_eg_port_if_set(pcf, ppe_drv_iface_ref(tx_port_if));
		ppe_drv_info("%p: No PPE interface corresponding\n", p);
		return true;
	}

	/*
	 * Should have a valid top interface.
	 */
	iface = top_iface = ppe_drv_iface_get_by_idx(top_if->tx_if);
	if (!iface) {
		ppe_drv_warn("%p: No PPE interface corresponding\n", p);
		return false;
	}

	/*
	 * Walk through if hierarchy.
	 */
	while (iface) {
		/*
		 * Routing to bridge device?
		 * Bridge's VSI and L3_if is the final egress VSI and egress L3_IF.
		 */
		if (iface->type == PPE_DRV_IFACE_TYPE_BRIDGE) {
			eg_vsi_if = iface;
			break;
		}

		if ((iface->type == PPE_DRV_IFACE_TYPE_VLAN)) {
			vlan_vsi = ppe_drv_iface_vsi_get(iface);
			if (vlan_vsi && ppe_drv_vsi_match_vlan(vlan_vsi, egress_vlan_inner, egress_vlan_outer)) {
				eg_vsi_if = iface;

				/*
				 * If desired eg_l3_if is also available, no need to continue the walk.
				 */
				if (eg_l3_if) {
					break;
				}
			}
		}

		if (iface->type == PPE_DRV_IFACE_TYPE_PPPOE) {
			pppoe_l3_if = ppe_drv_iface_l3_if_get(iface);
			if (pppoe_l3_if && ppe_drv_l3_if_pppoe_match(pppoe_l3_if, pcf->pppoe_session_id, pcf->pppoe_server_mac)) {
				eg_l3_if = iface;

				/*
				 * If desired eg_vsi_if is also available, no need to continue the walk.
				 */
				if (eg_vsi_if) {
					break;
				}
			}
		}

		iface = ppe_drv_iface_base_get(iface);
	}

	/*
	 * For create request with egress-VLAN, there must be a corresponding egress-VSI IF.
	 */
	if (ppe_drv_v4_conn_flow_egress_vlan_cnt_get(pcf) && !eg_vsi_if) {
		ppe_drv_warn("%p: not able to find a matching vlan-if", pcf);
		return false;
	}

	/*
	 * For create request with egress-pppoe, there must be a corresponding egress-L3 IF.
	 */
	if (ppe_drv_v4_conn_flow_flags_check(pcf, PPE_DRV_V4_CONN_FLOW_FLAG_PPPOE_FLOW) && !eg_l3_if) {
		ppe_drv_warn("%p: not able to find a matching pppoe-if", pcf);
		return false;
	}

	/*
	 * Take the reference on egress interfaces used for this flow.
	 * Dereference: connection destroy.
	 */
	pcf->eg_vsi_if = eg_vsi_if ? ppe_drv_iface_ref(eg_vsi_if) : NULL;
	pcf->eg_l3_if = eg_l3_if ? ppe_drv_iface_ref(eg_l3_if) : NULL;
	pcf->eg_port_if = ppe_drv_iface_ref(tx_port_if);
	return true;
}

/*
 * ppe_drv_v4_flow_check()
 *	Search an entry into the flow table and returns the flow object.
 */
static bool ppe_drv_v4_flow_check(struct ppe_drv_v4_conn_flow *pcf)
{
	struct ppe_drv_v4_5tuple tuple;
	struct ppe_drv_flow *flow = NULL;

	tuple.flow_ip = ppe_drv_v4_conn_flow_match_src_ip_get(pcf);
	tuple.flow_ident = ppe_drv_v4_conn_flow_match_src_ident_get(pcf);
	tuple.return_ip = ppe_drv_v4_conn_flow_match_dest_ip_get(pcf);
	tuple.return_ident = ppe_drv_v4_conn_flow_match_dest_ident_get(pcf);
	tuple.protocol = ppe_drv_v4_conn_flow_match_protocol_get(pcf);

	/*
	 * Get flow table entry.
	 */
	flow = ppe_drv_flow_v4_get(&tuple);
	if (!flow) {
		ppe_drv_info("%p: flow entry not found", pcf);
		return false;
	}

	ppe_drv_info("%p: flow found: index=%d host_idx=%d", pcf, flow->index, flow->host->index);

	return true;
}

/*
 * ppe_drv_v4_flow_del()
 *	Delete an entry from the flow table.
 */
static bool ppe_drv_v4_flow_del(struct ppe_drv_v4_conn_flow *pcf)
{
	struct ppe_drv *p = &ppe_drv_gbl;
	struct ppe_drv_flow *flow = pcf->pf;
	struct ppe_drv_fse_rule_info fse_info = {0};
	uint8_t service_code;
	uint32_t sawf_tag;
	struct ppe_drv_port *tx_port = NULL;
	struct ppe_drv_port *rx_port = NULL;

	ppe_drv_trace("%p, flow deletion request for flow-idx: %d host-idx: %u", pcf, flow->index, flow->host->index);

	/*
	 * As opposed to 'add', flow 'delete' needs host and flow entry to be handled separately.
	 * Note: a host entry or nexthop entry could be referenced by multiple flow entries.
	 */
	if (!ppe_drv_flow_del(flow)) {
		ppe_drv_warn("%p: flow entry deletion failed for flow_index: %d", flow, flow->index);
		return false;
	}

	/*
	 * Release host entry.
	 */
	ppe_drv_host_deref(flow->host);
	flow->host = NULL;

	/*
	 * Release nexthop entry.
	 */
	if (flow->nh) {
		ppe_drv_nexthop_deref(flow->nh);
		flow->nh = NULL;
	}

	/*
	 * Clear the QOS map information.
	 */
	ppe_drv_flow_v4_qos_clear(flow);

	/*
	 * Read stats and clear counters for deleted flow.
	 * Note: it is assured that no new flow can take the same index since all of this
	 * is lock protected. Unless this operation is complete, a new flow cannot be offloaded.
	 */
	ppe_drv_flow_v4_stats_update(pcf);

	ppe_drv_flow_stats_clear(flow);

	/*
	 * Update stats
	 */
	if (ppe_drv_v4_conn_flow_flags_check(pcf, PPE_DRV_V4_CONN_FLOW_FLAG_BRIDGE_FLOW)) {
		ppe_drv_stats_dec(&p->stats.gen_stats.v4_l2_flows);
	} else {
		ppe_drv_stats_dec(&p->stats.gen_stats.v4_l3_flows);
	}

	tx_port = ppe_drv_v4_conn_flow_tx_port_get(pcf);
	rx_port = ppe_drv_v4_conn_flow_rx_port_get(pcf);

	/*
	 * Update the number of VP and DS flows.
	 */
	if (ppe_drv_port_flags_check(tx_port, PPE_DRV_PORT_FLAG_WIFI_DEV) ||
			ppe_drv_port_flags_check(rx_port, PPE_DRV_PORT_FLAG_WIFI_DEV)) {
		ppe_drv_stats_dec(&p->stats.gen_stats.v4_vp_wifi_flows);
	}

	if ((tx_port->user_type == PPE_DRV_PORT_USER_TYPE_DS) ||
			(rx_port->user_type == PPE_DRV_PORT_USER_TYPE_DS)) {
		ppe_drv_stats_dec(&p->stats.gen_stats.v4_ds_flows);
	}

	/*
	 * Decrement flow count if sawf tag is valid.
	 */
	sawf_tag = PPE_DRV_SAWF_TAG_GET(pcf->sawf_mark);
	if (sawf_tag == PPE_DRV_SAWF_VALID_TAG) {
		service_code = PPE_DRV_SAWF_SERVICE_CLASS_GET(pcf->sawf_mark) + PPE_DRV_SC_SAWF_START;
		ppe_drv_stats_dec(&p->stats.sc_stats[service_code].sc_flow_count);
		ppe_drv_trace("Stats Updated: service code %u : flow  %llu\n",service_code, atomic64_read(&p->stats.sc_stats[service_code].sc_flow_count));
	}

	/*
	 * Clear the flow entry in SW so that it can be reused.
	 */
	flow->pkts = 0;
	flow->bytes = 0;
	flow->flags = 0;
	flow->service_code = 0;
	flow->type = 0;

	/*
	 * Delete corresponding FSE rule for a Wi-Fi flow.
	 */
	if (p->is_wifi_fse_up && ppe_drv_v4_conn_flow_flags_check(pcf, PPE_DRV_V4_CONN_FLOW_FLAG_FSE)) {
		ppe_drv_fill_fse_v4_tuple_info(pcf, &fse_info, false);

		if (p->fse_ops->destroy_fse_rule(&fse_info)) {
			ppe_drv_warn("%p: FSE v4 rule deletion failed\n", pcf);
			return true;
		}

		ppe_drv_v4_conn_flow_flags_clear(pcf, PPE_DRV_V4_CONN_FLOW_FLAG_FSE);
		kref_put(&p->fse_ops_ref, ppe_drv_fse_ops_free);
		ppe_drv_trace("%p: FSE v4 rule deletion successfull\n", pcf);
	}

	return true;
}

/*
 * ppe_drv_v4_flow_add()
 *	Adds an entry into the flow table and returns the flow index.
 */
static struct ppe_drv_flow *ppe_drv_v4_flow_add(struct ppe_drv_v4_conn_flow *pcf)
{
	struct ppe_drv *p = &ppe_drv_gbl;
	struct ppe_drv_nexthop *nh = NULL;
	struct ppe_drv_flow *flow = NULL;
	struct ppe_drv_host *host = NULL;
	struct ppe_drv_port *tx_port = NULL;
	struct ppe_drv_port *rx_port = NULL;

	/*
	 * Fetch a new nexthop entry.
	 * Note: NEXTHOP entry is not required for bridged flows.
	 */
	if (!ppe_drv_v4_conn_flow_flags_check(pcf, PPE_DRV_V4_CONN_FLOW_FLAG_BRIDGE_FLOW)) {
		nh = ppe_drv_nexthop_v4_get_and_ref(pcf);
		if (!nh) {
			ppe_drv_warn("%p: unable to allocate nexthop", pcf);
			return NULL;
		}
	}

	/*
	 * Add host table entry.
	 */
	host = ppe_drv_host_v4_add(pcf);
	if (!host) {
		ppe_drv_warn("%p: host entry add failed for conn flow", pcf);
		goto flow_add_fail;
	}

	/*
	 * Add flow table entry.
	 */
	flow = ppe_drv_flow_v4_add(pcf, nh, host, false);
	if (!flow) {
		ppe_drv_warn("%p: flow entry add failed for conn flow", pcf);
		goto flow_add_fail;
	}

	ppe_drv_info("%p: flow accelerated: index=%d host_idx=%d", pcf, flow->index, host->index);

	/*
	 * qos mapping can updated only after flow entry is added. It is added at the same index
	 * as that of flow entry.
	 */
	if (!ppe_drv_flow_v4_qos_set(pcf, flow)) {
		ppe_drv_warn("%p: qos mapping failed for flow: %p", pcf, flow);
		goto flow_add_fail;
	}

	/*
	 * Now the QOS mapping is set, mark the flow entry as valid.
	 */
	if (!ppe_drv_flow_valid_set(flow, true)) {
		ppe_drv_warn("%p: flow entry valid set failed for flow: %p", pcf, flow);
		goto flow_add_fail;
	}

	/*
	 * Update stats
	 */
	if (ppe_drv_v4_conn_flow_flags_check(pcf, PPE_DRV_V4_CONN_FLOW_FLAG_BRIDGE_FLOW)) {
		ppe_drv_stats_inc(&p->stats.gen_stats.v4_l2_flows);
	} else {
		ppe_drv_stats_inc(&p->stats.gen_stats.v4_l3_flows);
	}

	tx_port = ppe_drv_v4_conn_flow_tx_port_get(pcf);
	rx_port = ppe_drv_v4_conn_flow_rx_port_get(pcf);

	/*
	 * Update the number of VP and DS flows.
	 */
	if (ppe_drv_port_flags_check(tx_port, PPE_DRV_PORT_FLAG_WIFI_DEV) ||
			ppe_drv_port_flags_check(rx_port, PPE_DRV_PORT_FLAG_WIFI_DEV)) {
		ppe_drv_stats_inc(&p->stats.gen_stats.v4_vp_wifi_flows);
	}

	if ((tx_port->user_type == PPE_DRV_PORT_USER_TYPE_DS) ||
			(rx_port->user_type == PPE_DRV_PORT_USER_TYPE_DS)) {
		ppe_drv_stats_inc(&p->stats.gen_stats.v4_ds_flows);
	}

	ppe_drv_host_dump(flow->host);
	ppe_drv_flow_dump(flow);
	if (flow->nh) {
		ppe_drv_nexthop_dump(flow->nh);
	}

	return flow;

flow_add_fail:
	if (flow) {
		ppe_drv_flow_del(flow);
	}

	if (host) {
		ppe_drv_host_deref(host);
	}

	if (nh) {
		ppe_drv_nexthop_deref(nh);
	}

	return NULL;
}

/*
 * ppe_drv_v4_passive_vp_flow()
 *	check if the flow is for a Passive VP
 */
static bool ppe_drv_v4_passive_vp_flow(struct ppe_drv_v4_rule_create *create) {
	struct ppe_drv *p = &ppe_drv_gbl;
	struct ppe_drv_port *tx_pp = NULL;
	struct ppe_drv_port *rx_pp = NULL;
	struct ppe_drv_iface *if_rx, *if_tx;
	bool is_tx_vp = false;
	bool is_rx_vp = false;

	if_rx = ppe_drv_iface_get_by_idx(create->conn_rule.rx_if);
	if (!if_rx) {
		ppe_drv_warn("%p: No PPE interface corresponding to rx_if: %d", create, create->conn_rule.rx_if);
		return PPE_DRV_RET_FAILURE_INVALID_PARAM;
	}

	if_tx = ppe_drv_iface_get_by_idx(create->conn_rule.tx_if);
	if (!if_tx) {
		ppe_drv_warn("%p: No PPE interface corresponding to tx_if: %d", create, create->conn_rule.tx_if);
		return PPE_DRV_RET_FAILURE_INVALID_PARAM;
	}

	tx_pp = ppe_drv_iface_port_get(if_tx);
	if (!tx_pp) {
		ppe_drv_warn("%p: create failed invalid TX interface hierarchy: %p", p, create);
		return false;
	}

	rx_pp = ppe_drv_iface_port_get(if_rx);
	if (!rx_pp) {
		ppe_drv_warn("%p: create failed invalid RX interface hierarchy: %p", p, create);
		return false;
	}

	is_tx_vp = PPE_DRV_VIRTUAL_PORT_CHK(tx_pp->port);
	is_rx_vp = PPE_DRV_VIRTUAL_PORT_CHK(rx_pp->port);

	if (is_tx_vp || is_rx_vp) {
		if (is_tx_vp) {
			if (tx_pp->user_type == PPE_DRV_PORT_USER_TYPE_PASSIVE_VP)
				return true;
		}

		if (is_rx_vp) {
			if (rx_pp->user_type == PPE_DRV_PORT_USER_TYPE_PASSIVE_VP)
				return true;
		}
	}

	return false;
}

/*
 * ppe_drv_v4_conn_sync_one()
 *	Sync stats for a single connection.
 */
void ppe_drv_v4_conn_sync_one(struct ppe_drv_v4_conn *cn, struct ppe_drv_v4_conn_sync *cns,
		enum ppe_drv_stats_sync_reason reason)
{
	struct ppe_drv_v4_conn_flow *pcf = &cn->pcf;
	struct ppe_drv_v4_conn_flow *pcr = &cn->pcr;

	/*
	 * Fill 5-tuple connection rule from ppe_drv_v4_conn_flow to cns.
	 */
	cns->protocol = ppe_drv_v4_conn_flow_match_protocol_get(pcf);
	cns->flow_ip = ppe_drv_v4_conn_flow_match_src_ip_get(pcf);
	cns->flow_ident = ppe_drv_v4_conn_flow_match_src_ident_get(pcf);
	cns->return_ip = ppe_drv_v4_conn_flow_match_dest_ip_get(pcf);
	cns->return_ident = ppe_drv_v4_conn_flow_match_dest_ident_get(pcf);
	cns->flow_ip_xlate = ppe_drv_v4_conn_flow_xlate_src_ip_get(pcf);
	cns->flow_ident_xlate = ppe_drv_v4_conn_flow_xlate_src_ident_get(pcf);
	cns->return_ip_xlate = ppe_drv_v4_conn_flow_xlate_dest_ip_get(pcf);
	cns->return_ident_xlate = ppe_drv_v4_conn_flow_xlate_dest_ident_get(pcf);

	/*
	 * Fill reason for sync
	 */
	cns->reason = reason;

	/*
	 * Update stats for each direction
	 */
	ppe_drv_v4_conn_flow_rx_stats_get(pcf, &cns->flow_rx_packet_count, &cns->flow_rx_byte_count);
	ppe_drv_v4_conn_flow_tx_stats_get(pcf, &cns->flow_tx_packet_count, &cns->flow_tx_byte_count);
	ppe_drv_v4_conn_flow_rx_stats_sub(pcf, cns->flow_rx_packet_count, cns->flow_rx_byte_count);
	ppe_drv_v4_conn_flow_tx_stats_sub(pcf, cns->flow_tx_packet_count, cns->flow_tx_byte_count);

	/*
	 * Update the status for return flow, if it exist.
	 */
	if (ppe_drv_v4_conn_flags_check(cn, PPE_DRV_V4_CONN_FLAG_RETURN_VALID)) {
		ppe_drv_v4_conn_flow_rx_stats_get(pcr, &cns->return_rx_packet_count, &cns->return_rx_byte_count);
		ppe_drv_v4_conn_flow_tx_stats_get(pcr, &cns->return_tx_packet_count, &cns->return_tx_byte_count);
		ppe_drv_v4_conn_flow_rx_stats_sub(pcr, cns->return_rx_packet_count, cns->return_rx_byte_count);
		ppe_drv_v4_conn_flow_tx_stats_sub(pcr, cns->return_tx_packet_count, cns->return_tx_byte_count);

	} else {
		cns->return_rx_packet_count = 0;
		cns->return_rx_byte_count = 0;
		cns->return_tx_packet_count = 0;
		cns->return_tx_byte_count = 0;
	}
}

/*
 * ppe_drv_v4_conn_sync_many()
 *	API to sync a specific number of connection stats
 */
void ppe_drv_v4_conn_sync_many(struct ppe_drv_v4_conn_sync_many *cn_syn, uint8_t num_conn)
{
	uint8_t count = 0;
	bool return_flow_valid;
	struct ppe_drv *p = &ppe_drv_gbl;
	struct ppe_drv_v4_conn *cn;
	enum ppe_drv_stats_sync_reason reason = PPE_DRV_STATS_SYNC_REASON_STATS;
	uint16_t max_flow_conn_count = num_conn - PPE_DRV_TUN_PORT_STATS_RESERVED_COUNT;

	spin_lock_bh(&p->lock);
	if (list_empty(&p->conn_v4) && list_empty(&p->conn_tun_v4)) {
		spin_unlock_bh(&p->lock);
		return;
	}

	/*
	 * Traverse through active list of connection.
	 */
	list_for_each_entry(cn, &p->conn_v4, list) {
		/*
		 * Skip if
		 *	- Stats are already synced for this connection in previous iteration.
		 *	- Or there is no change in the stats from previous read.
		 */
		return_flow_valid = ppe_drv_v4_conn_flags_check(cn, PPE_DRV_V4_CONN_FLAG_RETURN_VALID);
		if ((cn->toggle == p->toggled_v4) || !(atomic_read(&cn->pcf.rx_packets)
				|| (return_flow_valid && atomic_read(&cn->pcr.rx_packets)))){
			if (list_is_last(&cn->list, &p->conn_v4)) {
				p->toggled_v4 = !p->toggled_v4;
				break;
			}

			continue;
		}

		/*
		 * sync stats for this connection.
		 */
		ppe_drv_v4_conn_sync_one(cn, &cn_syn->conn_sync[count], reason);
		count++;

		/*
		 * Flip the toggle bit to avoid syncing the stats for this connection until
		 * one full iteration of active list is done
		 */
		cn->toggle = !cn->toggle;

		/*
		 * If budget reached, break
		 */
		if (count == max_flow_conn_count) {
			break;
		}

		/*
		 * If we reached to the end of the list, flip the toggled bit
		 * for the next interation
		 */
		if (list_is_last(&cn->list, &p->conn_v4)) {
			p->toggled_v4 = !p->toggled_v4;
		}
	}

	/*
	 * Traverse through active list of tunnel connections.
	 */
	list_for_each_entry(cn, &p->conn_tun_v4, list) {
		/*
		 * Skip if stats are already synced for this connection in previous iteration.
		 */
		return_flow_valid = ppe_drv_v4_conn_flags_check(cn, PPE_DRV_V4_CONN_FLAG_RETURN_VALID);

		/*
		 * check if there are connections that need stats update; if yes then
		 * invoke stats_sync api for all those connections
		 */
		if (cn->toggle == p->tun_toggled_v4 || !(atomic_read(&cn->pcf.rx_packets) || atomic_read(&cn->pcf.tx_packets)
				|| (return_flow_valid && atomic_read(&cn->pcr.rx_packets))
				|| (return_flow_valid && atomic_read(&cn->pcr.tx_packets)))) {
			if (list_is_last(&cn->list, &p->conn_tun_v4)) {
				p->tun_toggled_v4 = !p->tun_toggled_v4;
				break;
			}

			continue;
		}

		/*
		 * sync stats for this connection.
		 */
		ppe_drv_v4_conn_sync_one(cn, &cn_syn->conn_sync[count], reason);
		count++;
		/*
		 * Flip the toggle bit to avoid syncing the stats for this connection until
		 * one full iteration of active list is done
		 */
		cn->toggle = !cn->toggle;

		/*
		 * If budget reached, break
		 */
		if (count == num_conn)
			break;

		/*
		 * If we reached to the end of the list, flip the toggled bit
		 * for the next interation
		 */
		if (list_is_last(&cn->list, &p->conn_tun_v4)) {
			p->tun_toggled_v4 = !p->tun_toggled_v4;
		}
	}

	spin_unlock_bh(&p->lock);
	cn_syn->count = count;
}
EXPORT_SYMBOL(ppe_drv_v4_conn_sync_many);

/*
 * ppe_drv_v4_conn_stats_sync_invoke_cb()
 *	Invoke cb
 */
void ppe_drv_v4_conn_stats_sync_invoke_cb(struct ppe_drv_v4_conn_sync *cns)
{
	struct ppe_drv *p = &ppe_drv_gbl;
	ppe_drv_v4_sync_callback_t sync_cb;
	void *sync_data;

	spin_lock_bh(&p->lock);
	sync_cb = p->ipv4_stats_sync_cb;
	sync_data = p->ipv4_stats_sync_data;
	spin_unlock_bh(&p->lock);

	if (sync_cb) {
		sync_cb(sync_data, cns);
		return;
	}

	ppe_drv_trace("%p: No callback registered for stats sync for cns: %p", p, cns);
}

/*
 * ppe_drv_v4_stats_callback_unregister()
 * 	Un-Register a notifier callback for IPv4 stats from PPE
 */
void ppe_drv_v4_stats_callback_unregister(void)
{
	struct ppe_drv *p = &ppe_drv_gbl;

	/*
	 * Unregister our sync callback.
	 */
	spin_lock_bh(&p->lock);
	if (p->ipv4_stats_sync_cb) {
		p->ipv4_stats_sync_cb = NULL;
		p->ipv4_stats_sync_data = NULL;
	}
	spin_unlock_bh(&p->lock);

	ppe_drv_info("%p: stats callback unregistered, cb", p);
}
EXPORT_SYMBOL(ppe_drv_v4_stats_callback_unregister);

/*
 * ppe_drv_v4_stats_callback_register()
 * 	Register a notifier callback for IPv4 stats from PPE
 */
bool ppe_drv_v4_stats_callback_register(ppe_drv_v4_sync_callback_t cb, void *app_data)
{
	struct ppe_drv *p = &ppe_drv_gbl;

	spin_lock_bh(&p->lock);
	p->ipv4_stats_sync_cb = cb;
	p->ipv4_stats_sync_data = app_data;
	spin_unlock_bh(&p->lock);

	ppe_drv_info("%p: stats callback registered, cb: %p", p, cb);
	return true;
}
EXPORT_SYMBOL(ppe_drv_v4_stats_callback_register);

/*
 * ppe_drv_v4_mc_destroy()
 *	Destroy a multicast connection entry in PPE.
 */
ppe_drv_ret_t ppe_drv_v4_mc_destroy(struct ppe_drv_v4_rule_destroy *destroy)
{
	return PPE_DRV_RET_FAILURE_NOT_SUPPORTED;
}

/*
 * ppe_drv_v4_mc_update()
 *	Update member list of ports in l2 multicast group.
 */
ppe_drv_ret_t ppe_drv_v4_mc_update(struct ppe_drv_v4_rule_create *update)
{
	return PPE_DRV_RET_FAILURE_NOT_SUPPORTED;
}

/*
 * ppe_drv_v4_mc_create()
 *	Adds a l2 multicast flow and host entry in PPE.
 */
ppe_drv_ret_t ppe_drv_v4_mc_create(struct ppe_drv_v4_rule_create *create)
{
	return PPE_DRV_RET_FAILURE_NOT_SUPPORTED;
}

/*
 * ppe_drv_v4_flush()
 *	flush a connection entry in PPE.
 */
ppe_drv_ret_t ppe_drv_v4_flush(struct ppe_drv_v4_conn *cn)
{
	struct ppe_drv *p = &ppe_drv_gbl;
	struct ppe_drv_v4_conn_flow *pcf = &cn->pcf;
	struct ppe_drv_v4_conn_flow *pcr = &cn->pcr;

	/*
	 * Update stats
	 */
	ppe_drv_stats_inc(&p->stats.gen_stats.v4_flush_req);

	/*
	 * Delete flow table entry.
	 */
	if (pcf && !ppe_drv_v4_flow_del(pcf)) {
		ppe_drv_stats_inc(&p->stats.gen_stats.v4_flush_fail);
		ppe_drv_warn("%p: deletion of flow failed: %p", p, pcf);
		return PPE_DRV_RET_FAILURE_FLUSH_FAIL;
	}

	/*
	 * Release references on interfaces.
	 */
	if (pcf) {
		ppe_drv_v4_if_walk_release(pcf);
	}

	/*
	 * Find the other flow associated with this connection.
	 */
	if (pcr && !ppe_drv_v4_flow_del(pcr)) {
		ppe_drv_stats_inc(&p->stats.gen_stats.v4_flush_fail);
		ppe_drv_warn("%p: deletion of return flow failed: %p", p, pcf);
		return PPE_DRV_RET_FAILURE_FLUSH_FAIL;
	}

	/*
	 * Release references on interfaces.
	 */
	if (pcr) {
		ppe_drv_v4_if_walk_release(pcr);
	}

	/*
	 * Delete connection entry to the active connection list.
	 */
	list_del(&cn->list);

	return PPE_DRV_RET_SUCCESS;
}

/*
 * ppe_drv_v4_rfs_destroy()
 *	Destroy a rfs connection entry in PPE.
 */
ppe_drv_ret_t ppe_drv_v4_rfs_destroy(struct ppe_drv_v4_rule_destroy *destroy)
{
	struct ppe_drv_comm_stats *comm_stats;
	struct ppe_drv *p = &ppe_drv_gbl;
	struct ppe_drv_flow *flow = NULL;
	struct ppe_drv_v4_conn_flow *pcf;
	struct ppe_drv_v4_conn *cn;

	comm_stats = &p->stats.comm_stats[PPE_DRV_CONN_TYPE_FLOW];

	/*
	 * Update stats
	 */
	ppe_drv_stats_inc(&comm_stats->v4_destroy_rfs_req);

	/*
	 * Get flow table entry.
	 */
	spin_lock_bh(&p->lock);
	flow = ppe_drv_flow_v4_get(&destroy->tuple);
	if (!flow) {
		spin_unlock_bh(&p->lock);
		ppe_drv_stats_inc(&comm_stats->v4_destroy_rfs_conn_not_found);
		ppe_drv_warn("%p: flow entry not found", p);
		return PPE_DRV_RET_FAILURE_DESTROY_NO_CONN;
	}

	pcf = flow->pcf.v4;
	cn = ppe_drv_v4_conn_flow_conn_get(pcf);

	if (!ppe_drv_v4_flow_del(pcf)) {
		spin_unlock_bh(&p->lock);
		ppe_drv_stats_inc(&comm_stats->v4_destroy_rfs_fail);
		ppe_drv_warn("%p: deletion of flow failed: %p", p, pcf);
		return PPE_DRV_RET_FAILURE_DESTROY_FAIL;
	}

	if (pcf->eg_port_if) {
		ppe_drv_iface_deref_internal(pcf->eg_port_if);
		pcf->eg_port_if = NULL;
	}

	spin_unlock_bh(&p->lock);

	/*
	 * Free the connection entry memory.
	 */
	ppe_drv_v4_conn_free(cn);

	return PPE_DRV_RET_SUCCESS;
}
EXPORT_SYMBOL(ppe_drv_v4_rfs_destroy);

/*
 * ppe_drv_v4_destroy()
 *	Destroy a connection entry in PPE.
 */
ppe_drv_ret_t ppe_drv_v4_destroy(struct ppe_drv_v4_rule_destroy *destroy)
{
	struct ppe_drv_comm_stats *comm_stats;
	struct ppe_drv *p = &ppe_drv_gbl;
	struct ppe_drv_flow *flow = NULL;
	struct ppe_drv_v4_conn_flow *pcf;
	struct ppe_drv_v4_conn_flow *pcr;
	struct ppe_drv_v4_conn_sync *cns;
	struct ppe_drv_v4_conn *cn;
	int ret;

	/*
	 * PPE accelearation is only supported for default port currently.
	 */
	if (ppe_drv_tun_check_support(destroy->tuple.protocol) || destroy->tuple.flow_ident == IANA_VXLAN_UDP_PORT || destroy->tuple.return_ident == IANA_VXLAN_UDP_PORT) {
		comm_stats = &p->stats.comm_stats[PPE_DRV_CONN_TYPE_TUNNEL];
		ppe_drv_stats_inc(&comm_stats->v4_destroy_req);
		ret = ppe_drv_v4_tun_del_ce_notify(destroy);
		if (ret != PPE_DRV_RET_SUCCESS) {
			ppe_drv_warn("%p: Tunnel destroy failed with error %d", destroy, ret);
			ppe_drv_stats_inc(&comm_stats->v4_destroy_fail);
			return ret;
		}

		return PPE_DRV_RET_SUCCESS;
	}

	comm_stats = &p->stats.comm_stats[PPE_DRV_CONN_TYPE_FLOW];

	/*
	 * Update stats
	 */
	ppe_drv_stats_inc(&comm_stats->v4_destroy_req);

	/*
	 * Get flow table entry.
	 */
	spin_lock_bh(&p->lock);
	flow = ppe_drv_flow_v4_get(&destroy->tuple);
	if (!flow) {
		spin_unlock_bh(&p->lock);
		ppe_drv_stats_inc(&comm_stats->v4_destroy_conn_not_found);
		ppe_drv_warn("%p: flow entry not found", p);
		return PPE_DRV_RET_FAILURE_DESTROY_NO_CONN;
	}

	pcf = flow->pcf.v4;

	if (!ppe_drv_v4_flow_del(pcf)) {
		spin_unlock_bh(&p->lock);
		ppe_drv_stats_inc(&comm_stats->v4_destroy_fail);
		ppe_drv_warn("%p: deletion of flow failed: %p", p, pcf);
		return PPE_DRV_RET_FAILURE_DESTROY_FAIL;
	}

	/*
	 * Release references on interfaces.
	 */
	ppe_drv_v4_if_walk_release(pcf);

	cn = ppe_drv_v4_conn_flow_conn_get(pcf);
	pcr = (pcf == &cn->pcf) ? &cn->pcr : &cn->pcf;

	/*
	 * Find the other flow associated with this connection.
	 */
	if (!ppe_drv_v4_flow_del(pcr)) {
		spin_unlock_bh(&p->lock);
		ppe_drv_stats_inc(&comm_stats->v4_destroy_fail);
		ppe_drv_warn("%p: deletion of return flow failed: %p", p, pcf);
		return PPE_DRV_RET_FAILURE_DESTROY_FAIL;
	}

	/*
	 * Release references on interfaces.
	 */
	ppe_drv_v4_if_walk_release(pcr);

	if (ppe_drv_tun_mapt_port_tun_get(pcf->tx_port, pcf->rx_port)) {
		if (!ppe_drv_tun_detach_mapt_v4_to_v6(cn)) {
			ppe_drv_trace("%p: mapt v4 to v6 detach failed", p);
		}
	}

	/*
	 * Delete connection entry from the active connection list.
	 */
	list_del(&cn->list);

	cns = ppe_drv_v4_conn_stats_alloc();
	if (cns) {
		ppe_drv_v4_conn_sync_one(cn, cns, PPE_DRV_STATS_SYNC_REASON_DESTROY);
	}

	spin_unlock_bh(&p->lock);

	/*
	 * Sync stats with ECM
	 */
	if (cns) {
		ppe_drv_v4_conn_stats_sync_invoke_cb(cns);
		ppe_drv_v4_conn_stats_free(cns);
	}

	/*
	 * We maintain reference per connection on main ppe context.
	 * Dereference: connection destroy.
	 *
	 * TODO: check if this is needed
	 */
	/* ppe_drv_deref(p); */

	/*
	 * Free the connection entry memory.
	 */
	ppe_drv_v4_conn_free(cn);

	return PPE_DRV_RET_SUCCESS;
}
EXPORT_SYMBOL(ppe_drv_v4_destroy);

/*
 * ppe_drv_v4_vxlan_tunnel()
 *	Check if create request for Vxlan tunnel.
 */
static bool ppe_drv_v4_vxlan_tunnel(struct ppe_drv_v4_rule_create *create)
{
	if ((create->tuple.protocol == IPPROTO_UDP) &&
		((create->tuple.flow_ident == IANA_VXLAN_UDP_PORT) ||
		(create->tuple.return_ident == IANA_VXLAN_UDP_PORT))) {
		return true;
	}

	return false;
}

/*
 * ppe_drv_v4_rfs_create()
 *	Adds a connection entry in PPE.
 */
ppe_drv_ret_t ppe_drv_v4_rfs_create(struct ppe_drv_v4_rule_create *create)
{
	struct ppe_drv *p = &ppe_drv_gbl;
	struct ppe_drv_v4_conn_flow *pcf = NULL;
	struct ppe_drv_comm_stats *comm_stats;
	struct ppe_drv_v4_conn *cn = NULL;
	ppe_drv_ret_t ret;

	comm_stats = &p->stats.comm_stats[PPE_DRV_CONN_TYPE_FLOW];

	/*
	 * Update stats
	 */
	ppe_drv_stats_inc(&comm_stats->v4_create_rfs_req);

	/*
	 * Allocate a new connection entry
	 */
	cn = ppe_drv_v4_conn_alloc();
	if (!cn) {
		ppe_drv_stats_inc(&comm_stats->v4_create_rfs_fail_mem);
		ppe_drv_warn("%p: failed to allocate connection memory: %p", p, create);
		return PPE_DRV_RET_FAILURE_CREATE_OOM;
	}

	/*
	 * Fill the connection entry.
	 */
	spin_lock_bh(&p->lock);

	ret = ppe_drv_v4_rfs_conn_fill(create, cn, PPE_DRV_CONN_TYPE_FLOW);
	if (ret != PPE_DRV_RET_SUCCESS) {
		ppe_drv_stats_inc(&comm_stats->v4_create_rfs_fail_conn);
		ppe_drv_warn("%p: failed to fill connection object: %p", p, create);
		spin_unlock_bh(&p->lock);
		kfree(cn);
		return ret;
	}

	/*
	 * Ensure either direction flow is not already offloaded by us.
	 */
	if (ppe_drv_v4_flow_check(&cn->pcf)) {
		ppe_drv_stats_inc(&comm_stats->v4_create_rfs_fail_collision);
		ppe_drv_warn("%p: create collision detected: %p", p, create);
		ret = PPE_DRV_RET_FAILURE_CREATE_COLLISSION;
		ppe_drv_iface_deref_internal(pcf->eg_port_if);
		spin_unlock_bh(&p->lock);
		kfree(cn);
		return ret;
	}

	pcf = &cn->pcf;

	/*
	 * Add flow direction flow entry
	 */
	pcf->pf = ppe_drv_v4_flow_add(pcf);
	if (!pcf->pf) {
		ppe_drv_stats_inc(&comm_stats->v4_create_rfs_fail);
		ppe_drv_warn("%p: acceleration of flow failed: %p", p, pcf);
		ret = PPE_DRV_RET_FAILURE_FLOW_ADD_FAIL;
		ppe_drv_iface_deref_internal(pcf->eg_port_if);
		spin_unlock_bh(&p->lock);
		kfree(cn);
		return ret;
	}

	ppe_drv_v4_conn_flow_flags_set(pcf, PPE_DRV_V4_CONN_FLAG_FLOW_PPE_ASSIST);
	pcf->conn = cn;

	cn->pcr.pf = NULL;
	spin_unlock_bh(&p->lock);

	return PPE_DRV_RET_SUCCESS;
}
EXPORT_SYMBOL(ppe_drv_v4_rfs_create);

/*
 * ppe_drv_v4_fse_flow_configure()
 *	FSE v4 flow programming
 */
static bool ppe_drv_v4_fse_flow_configure(struct ppe_drv_v4_rule_create *create, struct ppe_drv_v4_conn_flow *pcf,
					struct ppe_drv_v4_conn_flow *pcr)
{
	struct ppe_drv *p = &ppe_drv_gbl;
        struct ppe_drv_fse_rule_info fse_info = {0};
        struct ppe_drv_v4_conn_flow *fse_cn = NULL;
	struct ppe_drv_port *rx_port = ppe_drv_v4_conn_flow_rx_port_get(pcf);
	struct ppe_drv_port *tx_port = ppe_drv_v4_conn_flow_tx_port_get(pcf);
	bool is_tx_ds = (tx_port->user_type == PPE_DRV_PORT_USER_TYPE_DS);
	bool is_rx_ds = (rx_port->user_type == PPE_DRV_PORT_USER_TYPE_DS);
	bool is_tx_active_vp = (tx_port->user_type == PPE_DRV_PORT_USER_TYPE_ACTIVE_VP);
	bool is_rx_active_vp = (rx_port->user_type == PPE_DRV_PORT_USER_TYPE_ACTIVE_VP);

	/*
	 * Check if Connection manager is setting DS flag in the rule; if yes then decision
	 * space to choose pcf or pcr needs to be done only for DS VP else flow could also
	 * be for active VP so decision logic need to be executed for both active and DS VP.
	 */
	if ((create->rule_flags & PPE_DRV_V4_RULE_FLAG_DS_FLOW)  == PPE_DRV_V4_RULE_FLAG_DS_FLOW) {
		if (is_rx_ds && !is_tx_ds) {
			ppe_drv_fill_fse_v4_tuple_info(pcf, &fse_info, true);
			fse_cn = pcf;
		} else if (is_tx_ds && !is_rx_ds) {
			ppe_drv_fill_fse_v4_tuple_info(pcr, &fse_info, true);
			fse_cn = pcr;
		}
	} else if ((create->rule_flags & PPE_DRV_V4_RULE_FLAG_VP_FLOW)  == PPE_DRV_V4_RULE_FLAG_VP_FLOW) {
		if ((is_rx_ds && !is_tx_ds) || (is_rx_active_vp && !is_tx_active_vp)) {
			ppe_drv_fill_fse_v4_tuple_info(pcf, &fse_info, false);
			fse_cn = pcf;
		} else if ((is_tx_ds && !is_rx_ds) || (is_tx_active_vp && !is_rx_active_vp)) {
			ppe_drv_fill_fse_v4_tuple_info(pcr, &fse_info, false);
			fse_cn = pcr;
		}
	} else {
		if (is_rx_active_vp && !is_tx_active_vp) {
			ppe_drv_fill_fse_v4_tuple_info(pcf, &fse_info, false);
			fse_cn = pcf;
		} else if (is_tx_active_vp && !is_rx_active_vp) {
			ppe_drv_fill_fse_v4_tuple_info(pcr, &fse_info, false);
			fse_cn = pcr;
		} else if (is_rx_ds && !is_tx_ds) {
			ppe_drv_fill_fse_v4_tuple_info(pcf, &fse_info, true);
			fse_cn = pcf;
		} else if (is_tx_ds && !is_rx_ds) {
			ppe_drv_fill_fse_v4_tuple_info(pcr, &fse_info, true);
			fse_cn = pcr;
		}
	}

	if (p->fse_ops->create_fse_rule(&fse_info)) {
		ppe_drv_trace("%p: FSE rule configuration failed\n", p);
		return false;
	}

	ppe_drv_trace("%p: FSE rule configuration successful\n", p);
	ppe_drv_v4_conn_flow_flags_set(fse_cn, PPE_DRV_V4_CONN_FLOW_FLAG_FSE);
	return true;
}

/*
 * ppe_drv_v4_create()
 *	Adds a connection entry in PPE.
 */
ppe_drv_ret_t ppe_drv_v4_create(struct ppe_drv_v4_rule_create *create)
{
	struct ppe_drv *p = &ppe_drv_gbl;
	struct ppe_drv_v4_conn_flow *pcf = NULL;
	struct ppe_drv_v4_conn_flow *pcr = NULL;
	struct ppe_drv_comm_stats *comm_stats;
	struct ppe_drv_top_if_rule top_if;
	struct ppe_drv_v4_conn *cn = NULL;
	ppe_drv_ret_t ret;

	comm_stats = &p->stats.comm_stats[PPE_DRV_CONN_TYPE_FLOW];

	/*
	 * Check if PPE assist is required for the given interface; if yes
	 * then we should avoid accelerating these flows in PPE and should
	 * fallback to RFS specific API calls.
	 */
	spin_lock_bh(&p->lock);
	if (ppe_drv_v4_passive_vp_flow(create)) {
		spin_unlock_bh(&p->lock);
		ppe_drv_stats_inc(&comm_stats->v4_create_rfs_noedit_flow);
		ppe_drv_warn("%p: v4 Flow needs to be pushed through RFS API(s): %p", p, create);
		return PPE_DRV_RET_FAILURE_DUMMY_RULE;
	}

	spin_unlock_bh(&p->lock);

	/*
	 * PPE accelearation is only supported for default port currently.
	 */
	if (ppe_drv_tun_check_support(create->tuple.protocol) || ppe_drv_v4_vxlan_tunnel(create)) {
		comm_stats = &p->stats.comm_stats[PPE_DRV_CONN_TYPE_TUNNEL];
		ppe_drv_stats_inc(&comm_stats->v4_create_req);
		ret = ppe_drv_v4_tun_add_ce_notify(create);
		if (ret != PPE_DRV_RET_SUCCESS) {
			ppe_drv_stats_inc(&comm_stats->v4_create_fail);
			ppe_drv_warn("%p: failed to create tunnel CE with error %d", create, ret);
			return PPE_DRV_RET_FAILURE_TUN_CE_ADD_FAILURE;
		}

		return PPE_DRV_RET_SUCCESS;
	}

	/*
	 * Update stats
	 */
	ppe_drv_stats_inc(&comm_stats->v4_create_req);

	/*
	 * Allocate a new connection entry
	 */
	cn = ppe_drv_v4_conn_alloc();
	if (!cn) {
		ppe_drv_stats_inc(&comm_stats->v4_create_fail_mem);
		ppe_drv_warn("%p: failed to allocate connection memory: %p", p, create);
		return PPE_DRV_RET_FAILURE_CREATE_OOM;
	}

	/*
	 * Fill the connection entry.
	 */
	spin_lock_bh(&p->lock);
	ret = ppe_drv_v4_conn_fill(create, cn, PPE_DRV_CONN_TYPE_FLOW);
	if (ret != PPE_DRV_RET_SUCCESS) {
		ppe_drv_stats_inc(&comm_stats->v4_create_fail_conn);
		ppe_drv_warn("%p: failed to fill connection object: %p", p, create);
		goto fail;
	}

	/*
	 * Ensure either direction flow is not already offloaded by us.
	 */
	if (ppe_drv_v4_flow_check(&cn->pcf) || ppe_drv_v4_flow_check(&cn->pcr)) {
		ppe_drv_stats_inc(&comm_stats->v4_create_fail_collision);
		ppe_drv_warn("%p: create collision detected: %p", p, create);
		ret = PPE_DRV_RET_FAILURE_CREATE_COLLISSION;
		goto fail;
	}

	/*
	 * Perform interface hierarchy walk and obtain egress L3_If and egress VSI
	 * for each direction.
	 */
	top_if.rx_if = create->top_rule.rx_if;
	top_if.tx_if = create->top_rule.tx_if;
	if (!ppe_drv_v4_if_walk(&cn->pcf, &top_if, create->conn_rule.tx_if)) {
		ppe_drv_stats_inc(&comm_stats->v4_create_fail_if_hierarchy);
		ppe_drv_warn("%p: create failed invalid interface hierarchy: %p", p, create);
		ret = PPE_DRV_RET_FAILURE_INVALID_HIERARCHY;
		goto fail;
	}

	pcf = &cn->pcf;

	/*
	 * Reverse the top interfaces for return direction.
	 */
	top_if.rx_if = create->top_rule.tx_if;
	top_if.tx_if = create->top_rule.rx_if;
	if (!ppe_drv_v4_if_walk(&cn->pcr, &top_if, create->conn_rule.rx_if)) {
		ppe_drv_stats_inc(&comm_stats->v4_create_fail_if_hierarchy);
		ppe_drv_warn("%p: create failed invalid interface hierarchy: %p", p, create);
		ret = PPE_DRV_RET_FAILURE_INVALID_HIERARCHY;
		goto fail;
	}

	pcr = &cn->pcr;

	/*
	 * Add flow direction flow entry
	 */
	pcf->pf = ppe_drv_v4_flow_add(pcf);
	if (!pcf->pf) {
		ppe_drv_stats_inc(&comm_stats->v4_create_fail);
		ppe_drv_warn("%p: acceleration of flow failed: %p", p, pcf);
		ret = PPE_DRV_RET_FAILURE_FLOW_ADD_FAIL;
		goto fail;
	}

	pcr->pf = ppe_drv_v4_flow_add(pcr);
	if (!pcr->pf) {
		/*
		 * Destroy the offloaded flow entry
		 */
		ppe_drv_v4_flow_del(pcf);

		ppe_drv_stats_inc(&comm_stats->v4_create_fail);
		ppe_drv_warn("%p: acceleration of return direction failed: %p", p, pcr);
		ret = PPE_DRV_RET_FAILURE_FLOW_ADD_FAIL;
		goto fail;
	}

	/*
	 * Set the toggle bit to mark this connection as due for stats update in next sync.
	 */
	cn->toggle = !p->toggled_v4;

	/*
	 * Add connection entry to the active connection list.
	 */
	pcf->conn = cn;
	pcr->conn = cn;


	if (ppe_drv_tun_mapt_port_tun_get(pcf->tx_port, pcf->rx_port)) {
		if (!ppe_drv_tun_attach_mapt_v4_to_v6(cn)) {
			ppe_drv_trace("%p: mapt attach v4 to v6 failed", p);
		}
	}

	/*
	 * We maintain reference per connection on main ppe context.
	 * Dereference: connection destroy.
	 *
	 * TODO: check if this needed
	 */
	/* ppe_drv_ref(p); */

	/*
	 * Add corresponding FSE rule for a Wi-Fi flow.
	 */
	if (ppe_drv_fse_interface_check(pcf)) {
		if (!ppe_drv_v4_fse_flow_configure(create, pcf, pcr)) {
			/* TODO: Add a counter for this failure */
			ppe_drv_warn("%p: FSE flow table programming failed\n", p);
			goto fail;
		}

		kref_get(&p->fse_ops_ref);
	}

	list_add(&cn->list, &p->conn_v4);
	spin_unlock_bh(&p->lock);

	return PPE_DRV_RET_SUCCESS;

fail:
	if (pcf && pcf->pf) {
		ppe_drv_v4_flow_del(pcf);
	}

	if (pcr && pcr->pf) {
		ppe_drv_v4_flow_del(pcr);
	}

	/*
	 * Free flow direction references.
	 */
	if (pcf) {
		ppe_drv_v4_if_walk_release(pcf);
	}

	/*
	 * Free return direction references.
	 */
	if (pcr) {
		ppe_drv_v4_if_walk_release(pcr);
	}

	spin_unlock_bh(&p->lock);
	kfree(cn);
	return ret;
}
EXPORT_SYMBOL(ppe_drv_v4_create);

/*
 * ppe_drv_v4_nsm_stats_update()
 *	Update nsm stats for the given 5 tuple flow.
 */
bool ppe_drv_v4_nsm_stats_update(struct ppe_drv_nsm_stats *nsm_stats, struct ppe_drv_v4_5tuple *tuple)
{
	struct ppe_drv_flow *flow;
	struct ppe_drv *p = &ppe_drv_gbl;

	spin_lock_bh(&p->lock);
	flow = ppe_drv_flow_v4_get(tuple);

	if (!flow) {
		spin_unlock_bh(&p->lock);
		ppe_drv_warn("%p : Flow not found for given tuple information", tuple);
		return false;
	}

	nsm_stats->flow_stats.rx_bytes = flow->bytes;
	nsm_stats->flow_stats.rx_packets = flow->pkts;
	spin_unlock_bh(&p->lock);

	ppe_drv_trace("nsm stats : packets = %llu bytes = %llu", nsm_stats->flow_stats.rx_packets, nsm_stats->flow_stats.rx_bytes);
	return true;
}
EXPORT_SYMBOL(ppe_drv_v4_nsm_stats_update);
